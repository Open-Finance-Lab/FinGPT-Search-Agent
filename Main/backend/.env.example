# Django Settings
DJANGO_SECRET_KEY=your-secret-key-here-generate-new-one-for-production
DJANGO_DEBUG=True
DJANGO_ALLOWED_HOSTS=localhost,127.0.0.1

# FinGPT API Authentication
# When set, all /v1/* endpoints require: Authorization: Bearer <this-key>
# Leave empty to disable authentication (development only)
FINGPT_API_KEY=

# API Keys
OPENAI_API_KEY=your-openai-api-key-here
DEEPSEEK_API_KEY=your-deepseek-api-key-here
ANTHROPIC_API_KEY=your-anthropic-api-key-here
BUFFET_AGENT_API_KEY=your-buffet-agent-api-key-here
MEM0_API_KEY=your-mem0-api-key-here

# Optional Buffet Agent overrides (currently can only be called by using the Huggingface url)
# BUFFET_AGENT_ENDPOINT=https://l7d6yqg7nzbkumx8.us-east-1.aws.endpoints.huggingface.cloud
# BUFFET_AGENT_TIMEOUT=60

# MCP Server Configuration
MCP_SERVER_URL=http://127.0.0.1:9000/sse

# SEC EDGAR MCP Server (Required for SEC filing data access)
# Format: "Your Name (email@domain.com)" - Must be quoted if contains spaces
# Example: SEC_EDGAR_USER_AGENT="John Doe (john.doe@company.com)"
SEC_EDGAR_USER_AGENT="YourName (your.email@domain.com)"

# CORS Configuration
# Comma-separated list of allowed origins for CORS
CORS_ALLOWED_ORIGINS=chrome-extension://your-extension-id,http://localhost:3000,https://finance.yahoo.com,https://www.bloomberg.com,https://cdm.finos.org,https://www.finos.org,https://mathcup.com,https://www.cnbc.com

# Rate Limiting
# Format: "requests/period" where period can be 's' (second), 'm' (minute), 'h' (hour), 'd' (day)
# Examples: "100/h" = 100 requests per hour, "10/m" = 10 requests per minute
API_RATE_LIMIT=600/h

# Security Settings (production only)
# SESSION_COOKIE_SECURE=True
# CSRF_COOKIE_SECURE=True
# SECURE_SSL_REDIRECT=True

# Context Manager Configuration
# Choose between 'unified' (no compression) or 'mem0' (smart compression with 100k token limit)
CONTEXT_MANAGER_MODE=mem0

# Mem0 Context Manager Settings (only used when CONTEXT_MANAGER_MODE=mem0)
# Maximum tokens before triggering smart compression (default: 100000)
MEM0_CONTEXT_TOKEN_LIMIT=100000
# Target compression ratio when limit is reached (0.4 to 0.9, default: 0.7)
MEM0_COMPRESSION_TARGET_RATIO=0.7
# Maximum characters for compressed chunks (default: 4000)
MEM0_COMPRESSION_MAX_CHARS=4000

# Agent Streaming & Tracing
# These override per-model defaults in models_config.py.
# Leave unset to use the model's default (OpenAI: both true, Gemini: both false).
# AGENT_STREAMING=true
# AGENT_TRACING=true

# LLM Debug Logging
# Dumps the full payload sent to every foundation model call to Docker terminal
# Set LLM_DEBUG_VERBOSE=true to disable truncation and see full message content
LLM_DEBUG_LOG=false
# LLM_DEBUG_VERBOSE=false

# Optional: Intel MKL Library Conflict Resolution
# Set this if you encounter Intel MKL library conflicts (usually on certain CPU architectures)
# KMP_DUPLICATE_LIB_OK=TRUE

# Memory Monitoring
# Sliding window size for leak detection (number of requests)
MEMORY_LEAK_WINDOW_SIZE=200
# Slope threshold in MB/request to trigger leak alert
MEMORY_LEAK_SLOPE_THRESHOLD=0.1
# Check for leak trend every N requests
MEMORY_LEAK_CHECK_INTERVAL=50
# Soft limit: trigger graceful worker restart above this (MB)
MEMORY_SOFT_LIMIT_MB=450
# Token for /debug/memory/ endpoint (empty = disabled)
DEBUG_MEMORY_TOKEN=
# Stack frames stored per allocation in tracemalloc
TRACEMALLOC_FRAMES=25